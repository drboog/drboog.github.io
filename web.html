<!DOCTYPE html>
<html>
    <head>
    <style>
        body{
            padding-top: 5%;
            padding-left: 28%;
            padding-right: 28%;
            padding-bottom: 5%;
        }
    </style>
    </head>

    <body>
        <div id="main" padding="10%">
            <div id="page-head">
                <div id="photo"><img src="./index_files/yufan.jfif" align="right"></div>
                <h1>Yufan Zhou</h1>
                <p>Research Scientist at Adobe Research </p>
                <p>
                    <a href="https://www.linkedin.com/in/yufan-zhou-a1b21a156/"  target="_blank">[Linkedin]</a>,
                    <a href="https://scholar.google.com/citations?user=0eVrHJAAAAAJ"  target="_blank">[Google Scholar]</a>,
                    <a href="https://github.com/drboog"  target="_blank">[GitHub]</a>
                </p>
                <b>Email: </b> yufanzho AT buffalo DOT edu

            </div>

            <hr>
            <div class="section-heading"><h2>Bio</h2></div>

                <p>
                Currently my research focuses on generative models. More specifically, I'm interested in:
                </p>
                <ul>
                    <li>Multi-modal generative models (assistants) which are more user-friendly;</li>
                    <li>Customizing pre-trained generative models;</li>
                    <li>Saving the training or dataset construction cost in generative modeling;</li>
                </ul>


                <p>
                I obtained my Ph.D. from the <a href="https://engineering.buffalo.edu/computer-science-engineering.html"  target="_blank">Department of Computer Science and Engineering, University at Buffalo</a>, under the supervision of
                    <a href="https://cse.buffalo.edu/~jinhui/" target="_blank">Prof. Jinhui Xu</a> and
                    <a href="https://cse.buffalo.edu/~changyou/" target="_blank">Prof. Changyou Chen</a>.
                    I received my B.E. degree from <a href="https://www.zju.edu.cn/english/" target="_blank"> Zhejiang University</a>.
                    <br>
                </p>
                <p>
                    I worked as a Research Intern with <a href = "http://chunyuan.li/"  target="_blank">Chunyuan Li</a> (Microsoft), <a href = "https://zhangry868.github.io/"  target="_blank">Ruiyi Zhang</a> (Adobe), <a href="https://scholar.google.com/citations?user=uKdv6SUAAAAJ&hl=en"  target="_blank">Bingchen Liu</a> (ByteDance).
                </p>
                <p>
                    Self-motivated students who are interested in interning at Adobe, or seeking research collaborations, feel free to reach out to me.
                </p>

            <hr>
            <div class="section-heading"><h2>News</h2></div>
                <ul>
                    <li>One paper accepted by CoLM 2024.</li>
                    <li>One paper accepted by ACL 2024.</li>
                    <li>Two papers accepted by CVPR 2024.</li>
                    <li>One paper accepted by ICLR 2024.</li>
                </ul>
            <hr>

            <div class="section-heading"><h2>Selected Papers <a href="./papers.html">[More]</a></h2> </div>
                <ul>
                    <li>
                        <div class="paper">
                            <div class="title"><text id="small-caps"> <a href="https://arxiv.org/abs/2406.09305">Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven Text-to-Image Generation</a></text></div>
                            <div class="authors"> <b>Yufan Zhou</b>, Ruiyi Zhang, Kaizhi Zheng, Nanxuan Zhao, Jiuxiang Gu, Zichao Wang, Xin Eric Wang, Tong Sun. </div>
                            <div class="abs"><font color="#00539CFF"> Efficient method to construct dataset for subject-driven T2I generation, which can save at least tens of thousands of GPU hours.</font></i></div>
                        </div>
                    </li>
                    <br>

                    <li>
                        <div class="paper">
                            <div class="title"><text id="small-caps"> <a href="https://arxiv.org/abs/2312.03045">Customization Assistant for Text-to-Image Generation</a></text></div>
                            <div class="authors"> <b>Yufan Zhou</b>, Ruiyi Zhang, Jiuxiang Gu, Tong Sun. </div>
                            <div class="venue"> <i> IEEE Conference on Computer Vision and Pattern Recognition </i> (<b>CVPR</b>), 2024.</div>
                            <div class="abs"><font color="#00539CFF"> An assistant which can generate creative images for specific user-input subject along with text explanation and elaboration in 2-5 seconds, without any fine-tuning.</font></i></div>
                        </div>
                    </li>
                    <br>

                    <li>
                        <div class="paper">
                            <div class="title"><text id="small-caps"> <a href="https://arxiv.org/abs/2305.13579">Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach</a></text> <iframe src="https://ghbtns.com/github-btn.html?user=drboog&repo=ProFusion&type=star&count=true&size=small" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe></div>
                            <div class="authors"> <b>Yufan Zhou</b>, Ruiyi Zhang, Tong Sun, Jinhui Xu. </div>
                            <div class="abs"><font color="#00539CFF">A novel framework for customized text-to-image generation without the use of regularization. <br> We can efficiently customize a large-scale text-to-image generation model on single GPU, with only one image provided by the user.</font></i></div>

                        </div>
                    </li>
                    <br>

                    <li>
                        <div class="paper">
                            <div class="abs_photo"><img src="./index_files/hu.jpg" width="100" height="100" align="right"></div>
                            <div class="title"><text id="small-caps"> <a href="https://arxiv.org/abs/2211.15388">Shifted Diffusion for Text-to-image Generation</a></text> <iframe src="https://ghbtns.com/github-btn.html?user=drboog&repo=Shifted_Diffusion&type=star&count=true&size=small" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe> </div>
                            <div class="authors"> <b>Yufan Zhou</b>, Bingchen Liu, Yizhe Zhu, Xiao Yang, Changyou Chen, Jinhui Xu. </div>
                            <div class="venue"> <i> IEEE Conference on Computer Vision and Pattern Recognition </i> (<b>CVPR</b>), 2023.</div>
                            <div class="abs"> <font color="#00539CFF">We propose a method termed Corgi, which can better generate image embeddings from text inside multimodal embedding space. <br> It benefits both standard and language-free text-to-image generation. And yes, I do have a Corgi.</font> </i></div>
                        </div>
                    </li>
                    <br>

                    <li>
                        <div class="paper">
                            <div class="title"><text id="small-caps"> <a href="https://arxiv.org/abs/2111.13792">LAFITE: Towards Language-Free Training for Text-to-Image Generation</a></text> <iframe src="https://ghbtns.com/github-btn.html?user=drboog&repo=Lafite&type=star&count=true&size=small" frameborder="0" scrolling="0" width="150" height="20" title="GitHub"></iframe> </div>
                            <div class="authors"> <b>Yufan Zhou</b>, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, Tong Sun. </div>
                            <div class="venue"> <i> IEEE Conference on Computer Vision and Pattern Recognition </i> (<b>CVPR</b>), 2022.</div>
                            <div class="abs"><font color="#00539CFF">Our proposed work, Lafite, is the first work which can successfully train text-to-image generation model with image-only dataset.</font></i></div>
                        </div>
                    </li>
                    <br>

                    <li>
                        <div class="paper">
                            <div class="title"><text id="small-caps"> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20270">TiGAN: Text-Based Interactive Image Generation and Manipulation</a></text></div>
                            <div class="authors"> <b> Yufan Zhou</b>, Ruiyi Zhang, Jiuxiang Gu, Chris Tensmeyer, Tong Yu, Changyou Chen, Jinhui Xu, Tong Sun.</div>
                            <div class="venue"> <i>  AAAI conference on Artificial Intelligence </i> (<b>AAAI</b>), 2022.</div>
                        </div>
                    </li>
                    <br>

                    <li>
                        <div class="paper">
                            <div class="title"><text id="small-caps"> <a href = "https://arxiv.org/pdf/2102.03909.pdf"  target="_blank"> Meta-Learning with Neural Tangent Kernels</a></text></div>
                            <div class="authors"> <b>Yufan Zhou*</b>, Zhenyi Wang*, Jiayi Xian, Changyou Chen, Jinhui Xu.</div>
                            <div class="venue"> <i> International Conference on Learning Representations</i> (<b>ICLR</b>), 2021.</div>
                        </div>
                    </li>
                    <br>

                    <li>
                        <div class="paper">
                            <div class="title"><text id="small-caps"> <a href = "https://arxiv.org/pdf/2010.01761.pdf"  target="_blank">Learning Manifold Implicitly via Explicit Heat Kernel Learning</a></text></div>
                            <div class="authors"> <b>Yufan Zhou</b>, Changyou Chen, Jinhui Xu.</div>
                            <div class="venue"> <i> Conference on Neural Information Processing Systems</i> (<b>NeurIPS</b>), 2020.</div>
                        </div>
                    </li>

                </ul>

            <hr>
            <div class="section-heading"><h2>Professional Service</h2></div>
                <ul>
                    <li>
                        Conferences Program Committee/Reviewer: NeurIPS 2020, 2021, 2022, 2023; ICML 2021, 2022, 2023, 2024; ICLR 2022, 2023, 2024; CVPR 2023, 2024; AISTATS 2021; AAAI 2021, 2022; IJCAI 2021; EMNLP 2022, 2023; ECCV 2024; ACL 2023;
                    <li>
                        Journal Reviewer: IEEE Transactions on Neural Networks and learning systems;  IEEE Transactions on Circuits and Systems for Video Technology;
                    </li>

                </ul>
    </body>
</html>